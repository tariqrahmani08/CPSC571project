{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/calvin/git/cpsc571/notebook\n",
      "/Users/calvin/git/cpsc571/data\n"
     ]
    }
   ],
   "source": [
    "# Imports + data path definitions\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# required packages: scikit, nltk, pandas\n",
    "\n",
    "# / root\n",
    "#   / data\n",
    "#     / stopwords.txt\n",
    "#     / stock_corpus.csv\n",
    "#     / stockreddit\n",
    "#       / djia\n",
    "#       / scraped\n",
    "#     / stocknews\n",
    "#     / stockprices\n",
    "#       / ETFs\n",
    "#       / Stocks\n",
    "#     / stocktweets\n",
    "#       / preprocessed\n",
    "#       / processed\n",
    "#   / notebook\n",
    "\n",
    "DATA_PATH = r'../data'\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.path.realpath(DATA_PATH))\n",
    "\n",
    "REDDIT  = os.path.join(DATA_PATH, 'stockreddit')\n",
    "NEWS    = os.path.join(DATA_PATH, 'stocknews')\n",
    "PRICES  = os.path.join(DATA_PATH, 'stockprices')\n",
    "TWITTER = os.path.join(DATA_PATH, 'stocktweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the daily tweets into one file per stock index\n",
    "\n",
    "indices = ['AAPL', 'AMZN', 'FB']\n",
    "output = os.path.join(TWITTER, 'processed')\n",
    "\n",
    "for index in indices:\n",
    "    path = os.path.join(TWITTER, 'preprocessed', index)\n",
    "    dfs = []\n",
    "    for file in os.listdir(path):\n",
    "        dfs.append(pd.read_json(os.path.join(path, file), orient='records', lines=True))\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    df.to_json(os.path.join(output, f'{index}.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all different data sources + header standardization\n",
    "\n",
    "reddit = pd.read_csv(os.path.join(REDDIT, 'scraped', 'amazon_reddit.csv'))\n",
    "reddit.rename({'Title': 'text', 'Publish Date': 'date'}, axis=1, inplace=True)\n",
    "reddit['date'] = pd.to_datetime(reddit['date'])\n",
    "reddit['source'] = pd.Series('reddit', reddit.index)\n",
    "\n",
    "news = pd.read_csv(os.path.join(NEWS, 'Amazon.csv'))\n",
    "news.rename({'newsHeadline': 'text', 'start_time_stamp': 'date'}, axis=1, inplace=True)\n",
    "news['date'] = pd.to_datetime(news['date'])\n",
    "news['source'] = pd.Series('news', news.index)\n",
    "\n",
    "tweets = pd.read_json(os.path.join(TWITTER, 'processed', 'AMZN.json'), lines=True)\n",
    "tweets.rename({'created_at': 'date', 'user_id_str': 'user_id'}, axis=1, inplace=True)\n",
    "tweets['text'] = tweets['text'].apply(lambda x: ' '.join(x))\n",
    "tweets['source'] = pd.Series('twitter', tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet text before joining with Reddit and News\n",
    "\n",
    "import re\n",
    "\n",
    "stopwords = open(os.path.join(DATA_PATH, 'stopwords.txt'), 'r') \\\n",
    "                .read() \\\n",
    "                .split('\\n')\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\s\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^rt[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "tweets['text'] = tweets['text'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final stock corpus\n",
    "\n",
    "corpus = pd.concat([reddit, news, tweets], axis=0, join='inner') \\\n",
    "            .sort_values(by='date') \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "corpus.to_csv(os.path.join(DATA_PATH, 'stock_corpus.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# http://blog.chapagain.com.np/python-nltk-twitter-sentiment-analysis-natural-language-processing-nlp/\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print (len(pos_tweets)) # Output: 5000\n",
    " \n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print (len(neg_tweets)) # Output: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    "\n",
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    " \n",
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    " \n",
    "print (len(pos_tweets_set), len(neg_tweets_set)) # Output: (5000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    " \n",
    "print(len(test_set),  len(train_set)) # Output: (2000, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
